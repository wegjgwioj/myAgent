这是一个关于**“最小二乘法、MSE 与深度学习”的完整知识图谱梳理。

---

### 一、 核心结论 (The Core)
**一句话总结：** 在深度学习中，当我们使用 **MSE (均方误差)** 作为损失函数时，我们本质上是在执行 **最小二乘法**，且隐含了**“数据噪声服从高斯分布（正态分布）”**这一统计学假设。

* **公式表现：**
    $$L = \sum (y_{true} - y_{pred})^2$$
* **代码表现：** `criterion = nn.MSELoss()`

---

### 二、 三个维度的深度解析

#### 1. 几何维度 (直觉)
* **目的：** 寻找一条线（或超平面），使得所有观测点到这条线的**垂直距离**（欧氏距离）最小。
* **为什么是平方？** 欧氏距离公式是 $\sqrt{\sum(y-\hat{y})^2}$。为了计算方便（去掉根号不影响单调性），我们直接最小化距离的平方。
* **关键词：** 垂直投影、欧氏距离。



#### 2. 概率维度 (本质)
这是最底层的解释，解释了“为什么是平方而不是绝对值”。

* **假设：** 现实世界的噪声 $\epsilon$ 服从**高斯分布** (Gaussian Distribution)。
    $$P(\epsilon) \propto e^{-\epsilon^2}$$
* **推导逻辑 (极大似然估计 MLE)：**
    1.  **目标：** 让模型预测出当前数据的概率最大化。
    2.  **过程：**
        * 最大化 概率 $P$
        * $\Leftrightarrow$ 最大化 $\log P$ (取对数把乘法变加法)
        * $\Leftrightarrow$ 最大化 $-\sum (y - \hat{y})^2$ (因为高斯公式指数上有个负号)
        * $\Leftrightarrow$ **最小化** $\sum (y - \hat{y})^2$
* **结论：** **MSE Loss 等价于高斯噪声下的极大似然估计。**

#### 3. 优化维度 (手段)
虽然原理一样，但求解手段不同：
* **传统统计学：** 试图求**解析解**（一步到位），使用正规方程 $(X^TX)^{-1}X^Ty$。
    * *缺点：* 矩阵求逆太慢，无法处理大数据。
* **深度学习：** 求**数值解**（一步步挪），使用**梯度下降 (Gradient Descent)**。
    * *优点：* 适合海量数据和非线性模型。
    * *联系：* 最小二乘法的函数是一个完美的“碗”型（凸函数），这保证了梯度下降一定能滑到底部（全局最优解）。

---

### 三、 知识对比表 (Contextualizing)

为了让你知道什么时候用它，什么时候**不**用它，请看下表：

| 特性 | **MSE / 最小二乘法** | **MAE / 平均绝对误差** |
| :--- | :--- | :--- |
| **数学形式** | $(y - \hat{y})^2$ | $|y - \hat{y}|$ |
| **隐含假设** | **高斯分布** (常见噪声) | **拉普拉斯分布** (尖峰厚尾) |
| **对异常值** | **非常敏感** (平方会放大错误) | **鲁棒性强** (不怎么受离群点影响) |
| **适用场景** | 回归任务 (房价、坐标预测) | 数据有很多脏点/离群点时 |
| **导数性质** | 处处可导 (梯度下降很顺滑) | 在0点不可导 (优化稍麻烦) |

---

### 四、 

MSE 对应高斯分布假设。如果以后你的模型不管怎么调 MSE 都降不下来，记得检查一下数据是不是有极端的离群点（Outliers），因为 MSE 会被这些点带偏。



* **MSE** $\Leftrightarrow$ 假设是 **高斯分布**
* **Cross-Entropy** $\Leftrightarrow$ 假设是 **伯努利分布 (二分类)** 或 **多项式分布 (多分类)**

