# 线性回归模型 (Linear Regression) 学习笔记

## 1. 简介
线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。
其核心思想是：**试图学得一个线性模型以尽可能准确地预测实值输出标记。**

### 分类
- **一元线性回归**：涉及一个自变量。
- **多元线性回归**：涉及两个或两个以上自变量。

---

## 2. 核心原理

### 2.1 假设函数 (Hypothesis)
对于一个样本 $x = (x_1, x_2, ..., x_n)$，线性回归模型试图学得：
$$ f(x) = w_1x_1 + w_2x_2 + ... + w_nx_n + b $$
写成向量形式：
$$ h_\theta(x) = \theta^T x $$
其中 $\theta$ 是参数向量（包含权重 $w$ 和偏置 $b$），$x$ 是特征向量（通常为了计算方便，会增加 $x_0=1$）。

### 2.2 损失函数 (Cost Function)
为了衡量模型预测值与真实值之间的差异，我们通常使用**均方误差 (Mean Squared Error, MSE)** 作为损失函数：
$$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 $$
其中：
- $m$ 是样本数量
- $x^{(i)}$ 是第 $i$ 个样本的特征
- $y^{(i)}$ 是第 $i$ 个样本的真实标签
- 系数 $\frac{1}{2}$ 是为了方便求导计算。

### 2.3 优化目标
我们的目标是找到一组参数 $\theta$，使得损失函数 $J(\theta)$ 最小：
$$ \min_\theta J(\theta) $$

### 2.4 求解方法
1.  **梯度下降法 (Gradient Descent)**：
    通过迭代的方式更新参数，沿着损失函数梯度的反方向移动，直到达到局部最小值。
    更新公式：
    $$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) $$
    其中 $\alpha$ 是学习率。

2.  **正规方程 (Normal Equation)**：
    直接通过数学推导求出解析解（适用于特征数量不是特别大的情况）：
    $$ \theta = (X^T X)^{-1} X^T y $$

---

## 3. 经典案例：房屋价格预测

这是线性回归最经典的入门案例。假设我们有一组房屋数据，包含房屋面积（特征 $x$）和房屋价格（标签 $y$）。

### 场景描述
我们希望根据房屋的面积来预测其销售价格。

| 房屋面积 (平方英尺) $x$ | 价格 (千美元) $y$ |
| :--- | :--- |
| 2104 | 400 |
| 1600 | 330 |
| 2400 | 369 |
| 1416 | 232 |
| ... | ... |

### 建模思路
1.  **特征选择**：将“房屋面积”作为输入特征 $x$。
2.  **模型假设**：假设价格与面积呈线性关系 $y = wx + b$。
3.  **训练**：使用已有的数据（训练集）来拟合直线，找到最佳的 $w$ 和 $b$。
4.  **预测**：对于新房子的面积，代入公式计算预测价格。

---



### 总结
线性回归虽然简单，但它是许多复杂模型（如神经网络）的基础。理解其背后的损失函数和优化思想非常重要。

