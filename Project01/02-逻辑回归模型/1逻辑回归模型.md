**逻辑回归 (Logistic Regression)** 尽管名字里带有“回归”二字，但它实际上是机器学习中最基础、最重要的 **分类算法**。

-----


## 1\. 核心概念：它是什么？

逻辑回归是一种用于解决 二分类（Binary Classification） 问题的统计模型。

  * **线性回归 vs. 逻辑回归：**
      * **线性回归**：预测连续的值（例如：根据面积预测房价）。
      * **逻辑回归**：预测事件发生的概率（例如：这封邮件是垃圾邮件的概率是多少？）。

**直观理解：**
想象你在做一个通过考试的预测。线性回归可能会输出“你的分数是 85 分”；而逻辑回归会告诉你“你有 92% 的概率通过考试”，并且如果概率大于 50%，系统就判定为“通过”。

-----

## 2\. 数学原理 (The Engine)

逻辑回归由两部分组成：**线性部分** 和 **激活函数**。

### 2.1 线性函数

首先，像线性回归一样，计算特征的加权和：
$$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$
或者写成向量形式：
$$z = w^T x + b$$

### 2.2 Sigmoid 激活函数

线性结果 $z$ 的范围是 $(-\infty, +\infty)$，但这不能表示概率。我们需要将其压缩到 $(0, 1)$ 之间。这时就需要 **Sigmoid 函数**：

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

  * 当 $z$ 很大时，$\sigma(z) \approx 1$。
  * 当 $z$ 很小时，$\sigma(z) \approx 0$。
  * 当 $z = 0$ 时，$\sigma(z) = 0.5$（决策边界）。

### 2.3 预测与决策

模型输出的是属于正类（例如：患病、垃圾邮件）的概率：
$$\hat{y} = P(y=1|x)$$

  * 如果 $\hat{y} \ge 0.5$，预测为类别 1。
  * 如果 $\hat{y} < 0.5$，预测为类别 0。

-----

## 3\. 损失函数 (Loss Function)

我们如何训练模型找到最佳的权重 $w$ 和偏置 $b$？
在线性回归中我们使用均方误差 (MSE)，但在逻辑回归中，MSE 会导致损失函数非凸（有多个局部最小值），难以优化。

因此，我们使用 **对数损失 (Log Loss)**，也叫 **二元交叉熵 (Binary Cross-Entropy)**：

$$J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)})]$$

  * 如果真实值 $y=1$，我们需要 $\hat{y}$ 越接近 1 越好（损失接近 0）。
  * 如果真实值 $y=0$，我们需要 $\hat{y}$ 越接近 0 越好。

**优化方法：** 通常使用 **梯度下降 (Gradient Descent)** 来最小化这个损失函数。

-----

## 4\. 实战代码演练 (Python + Scikit-Learn)

下面是一个完整的代码示例，使用经典的乳腺癌数据集（二分类问题）来训练逻辑回归模型。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns

# 1. 加载数据
data = load_breast_cancer()
X = data.data
y = data.target

# 2. 数据集切分 (80% 训练, 20% 测试)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. 特征标准化 (非常重要！)
# 逻辑回归对特征的尺度很敏感，必须进行标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. 初始化并训练模型
# C是正则化强度的倒数，C越小正则化越强（防止过拟合）
model = LogisticRegression(random_state=42, max_iter=1000) 
model.fit(X_train_scaled, y_train)

# 5. 预测
y_pred = model.predict(X_test_scaled)
y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] # 获取预测为正类的概率

# 6. 评估模型
accuracy = accuracy_score(y_test, y_pred)
print(f"模型准确率 (Accuracy): {accuracy:.4f}")
print("\n分类报告:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

# 7. 可视化混淆矩阵
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# (可选) 查看特征重要性
# 逻辑回归的一个巨大优势是可解释性，我们可以看权重的绝对值大小
coefficients = model.coef_[0]
feature_importance = sorted(zip(coefficients, data.feature_names), key=lambda x: abs(x[0]), reverse=True)

print("\n前5个最重要的特征 (根据权重):")
for coef, feat in feature_importance[:5]:
    print(f"{feat}: {coef:.4f}")
```

-----

## 5\. 模型评估与核心指标

除了准确率（Accuracy），逻辑回归通常还需要关注以下指标，特别是在数据不平衡时：

1.  **混淆矩阵 (Confusion Matrix):** 直观展示预测对错的分布。
2.  **精确率 (Precision):** 预测为正类的样本中，有多少是真的正类？（宁缺毋滥）。
3.  **召回率 (Recall):** 所有的正类样本中，找出了多少？（宁可错杀一千，不可放过一个）。
4.  **ROC 曲线与 AUC 值:** 衡量模型在不同阈值下的泛化能力。AUC 越接近 1 越好。

-----

## 6\. 逻辑回归的优缺点

| 优点 | 缺点 |
| :--- | :--- |
| **简单高效**：计算量小，速度极快，适合作为 Baseline。 | **线性局限**：本质上是一个线性分类器，无法解决非线性问题（如异或问题），除非手动构造特征。 |
| **可解释性强**：可以通过权重看到哪个特征对结果影响最大。 | **特征敏感**：对异常值和多重共线性（特征间高度相关）敏感。 |
| **输出概率**：不仅给分类结果，还给置信度。 | **依赖特征工程**：需要良好的数据预处理（如归一化）。 |

-----

## 7\. 进阶：多分类问题 (Multinomial LR)

虽然逻辑回归默认是二分类，但它可以通过以下两种方式处理多分类（如手写数字识别 0-9）：

1.  **OVR (One-vs-Rest):** 对 $N$ 个类别训练 $N$ 个分类器（例如：是“0”还是“非0”）。
2.  **Softmax 回归:** 将 Sigmoid 函数推广为 **Softmax 函数**，使所有类别的概率之和为 1。Scikit-learn 中的 `multi_class='multinomial'` 参数即可开启此模式。

-----

### 总结

逻辑回归是机器学习领域的“瑞士军刀”。尽管现在有更复杂的模型（如随机森林、神经网络），但在工业界，**逻辑回归依然是点击率预测（CTR）、风险控制（评分卡模型）和医疗诊断的首选模型之一**，因为它的解释性是黑盒模型无法比拟的。
