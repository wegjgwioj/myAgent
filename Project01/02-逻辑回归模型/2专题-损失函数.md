损失函数（Loss Function）的定义是根据我们希望模型达到的**目标**以及模型本身的**输出形式**来确定的。

对于逻辑回归（Logistic Regression），我们不使用线性回归常用的**均方误差 (MSE)**，而是使用专门针对**分类问题**和**概率输出**设计的**对数损失 (Log Loss)**，也称为**二元交叉熵 (Binary Cross-Entropy)**。

下面将详细解释为什么要定义 Log Loss，以及它是如何通过数学原理构建出来的。

---

## 1. 为什么不能用均方误差（MSE）？

在线性回归中，我们希望预测值 $\hat{y}$ 与真实值 $y$ 的距离越小越好，MSE 定义为：
$$MSE = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}^{(i)} - y^{(i)})^2$$

如果我们将 MSE 用在逻辑回归上，会遇到一个致命问题：**优化困难**。

回忆一下逻辑回归的预测值 $\hat{y}$ 是 Sigmoid 函数 $\sigma(z)$ 的输出，即 $\hat{y} = \sigma(w^T x + b)$。

如果我们将 $\hat{y}$ 代入 MSE 公式中，损失函数 $J(w, b)$ 会变成一个关于 $w$ 和 $b$ 的**非凸函数 (Non-Convex)**。



* **非凸函数**：损失曲面上存在许多“山谷”（局部最小值，Local Minima）。
* **后果**：当我们使用梯度下降法进行优化时，模型可能会陷入某一个局部最小值，而不是找到真正的全局最小值，导致训练失败。

为了确保梯度下降能找到最优解，我们需要一个**凸函数 (Convex)** 形式的损失函数。

---

## 2. 对数损失（Log Loss）的数学构建

对数损失（Binary Cross-Entropy）的设计灵感来源于**最大似然估计 (Maximum Likelihood Estimation, MLE)**。

### 2.1 最大似然估计 (MLE)

MLE 的思想是：模型参数应该让**当前观察到的所有样本出现的概率最大化**。

在逻辑回归中，我们假设样本 $y$ 服从伯努利分布（Bernoulli Distribution），即二项分布：

* 当真实标签 $y=1$ 时，样本出现的概率为 $\hat{y}$。
    $$P(y=1 | x) = \hat{y}$$
* 当真实标签 $y=0$ 时，样本出现的概率为 $1 - \hat{y}$。
    $$P(y=0 | x) = 1 - \hat{y}$$

我们可以将这两个概率合并成一个简洁的表达式：
$$P(y | x) = \hat{y}^y \cdot (1 - \hat{y})^{(1 - y)}$$

* 如果 $y=1$，上式等于 $\hat{y}^1 \cdot (1 - \hat{y})^0 = \hat{y}$。
* 如果 $y=0$，上式等于 $\hat{y}^0 \cdot (1 - \hat{y})^1 = 1 - \hat{y}$。

### 2.2 构建似然函数与损失函数

假设所有 $m$ 个训练样本 $(x^{(i)}, y^{(i)})$ 都是独立同分布的，那么整个训练集出现的**联合概率（似然函数）**是每个样本概率的乘积：

$$L(w, b) = \prod_{i=1}^{m} P(y^{(i)} | x^{(i)}) = \prod_{i=1}^{m} [\hat{y}^{(i)}]^{y^{(i)}} [1 - \hat{y}^{(i)}]^{(1 - y^{(i)})}$$

我们的目标是**最大化**这个似然函数 $L$。

### 2.3 转化为损失函数（最小化）

在优化中，我们习惯于**最小化**损失函数，而不是最大化似然函数。

1.  **取对数 (Logarithm)：** 为了将乘积操作转化为求和操作（简化求导），我们对似然函数取对数，得到**对数似然** $\log(L)$：
    $$\log(L) = \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})]$$

2.  **取负号 (Negative)：** 为了将“最大化 $\log(L)$”转化为“最小化损失”，我们给 $\log(L)$ 加上负号，并除以样本数 $m$ 进行平均，最终得到 **对数损失 (Log Loss)**：

$$J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1 - y^{(i)}) \log(1 - \hat{y}^{(i)})]$$

---

## 3. 对数损失的性质

Log Loss 完美地达到了我们的目标：

1.  **凸性 (Convexity)：** 当 $y=1$ 时， $-\log(\hat{y})$ 的函数是凸的；当 $y=0$ 时，$-\log(1-\hat{y})$ 的函数是凸的。这保证了**梯度下降可以找到全局最优解**。
2.  **惩罚性 (Penalty)：**
    * 如果 $y=1$ 且 $\hat{y} \to 0$（预测错了，而且错得很离谱），损失项 $-\log(\hat{y})$ 会趋于**无穷大**，给予模型极大的惩罚。
    * 如果 $y=1$ 且 $\hat{y} \to 1$（预测对了），损失项趋于 $-\log(1)=0$，惩罚极小。

正是这种从最大似然估计推导出的定义，使得逻辑回归的训练在数学上高效且鲁棒。
