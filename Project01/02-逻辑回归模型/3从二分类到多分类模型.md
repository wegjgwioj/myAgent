
在多分类问题中，逻辑回归的对数损失（Binary Cross-Entropy）通过引入 **Softmax 函数** 和 **分类交叉熵损失（Categorical Cross-Entropy Loss）** 来实现扩展。

---

## 1. Softmax 函数：多分类的概率输出

在二分类逻辑回归中，我们使用 **Sigmoid 函数** 将线性输出 $z$ 压缩到 $(0, 1)$ 区间，表示为属于正类的概率 $P(y=1|x)$。

在多分类问题（假设有 $K$ 个类别）中，我们需要为每个类别 $k \in \{1, 2, \dots, K\}$ 都计算一个概率 $P(y=k|x)$，并且要求所有类别的概率和等于 1。

这就是 **Softmax 函数** 的作用。

### 1.1 线性得分 $z_k$

首先，模型会为每个类别 $k$ 计算一个线性得分（或称为 **logits**）$z_k$：
$$z_k = w_k^T x + b_k$$
这里，$w_k$ 和 $b_k$ 是对应第 $k$ 个类别的权重和偏置。

### 1.2 Softmax 激活

Softmax 函数将所有类别的线性得分 $z_k$ 转化为概率 $\hat{y}_k$：

$$\hat{y}_k = P(y=k|x) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}$$

* **特点：**
    * 分子的 $e^{z_k}$ 确保了概率都是正数。
    * 分母是所有类别的指数得分之和，确保了 $\sum_{k=1}^{K} \hat{y}_k = 1$。

Softmax 输出了一个包含 $K$ 个元素的向量 $\hat{\mathbf{y}}$，其中每个元素 $\hat{y}_k$ 代表了样本属于第 $k$ 个类别的概率。



---

## 2. 分类交叉熵损失 (Categorical Cross-Entropy Loss)

在多分类问题中，我们使用分类交叉熵损失，它同样来源于**最大似然估计**，是对二元交叉熵的自然扩展。

### 2.1 独热编码 (One-Hot Encoding)

在多分类问题中，真实标签 $y$ 通常被表示为**独热编码（One-Hot Encoding）**的向量 $\mathbf{y}$。这是一个包含 $K$ 个元素的向量，其中只有一个元素是 1（代表正确的类别），其余都是 0。

例如，如果有 3 个类别，样本 $x$ 的真实类别是 2：
$$\mathbf{y} = [0, 1, 0]$$

### 2.2 损失函数的定义

分类交叉熵损失 $J$ 定义如下：
$$J(\mathbf{w}, \mathbf{b}) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log(\hat{y}_k^{(i)})$$

**核心解析：**

1.  **内层求和 $\sum_{k=1}^{K}$ (针对类别):** 由于 $\mathbf{y}^{(i)}$ 是独热编码，在内层求和中，**只有真实类别对应的 $y_k^{(i)}$ 为 1，其余均为 0**。

2.  **简化后的损失项：**
    因此，对于每个样本 $i$，上式实际上只保留了正确类别 $c$ 上的损失：
    $$-\log(\hat{y}_c^{(i)})$$

3.  **目标：**
    * 我们的目标是最小化 $J$，也就是最大化 $\log(\hat{y}_c)$，最终目标是最大化 $\hat{y}_c$。
    * 这意味着模型需要努力让**正确类别 $c$ 上的概率 $\hat{y}_c$ 尽可能地接近 1**。

**二元交叉熵是它的特例：**
当 $K=2$ 时，分类交叉熵：
$$J = - \sum_{k=1}^{2} y_k \log(\hat{y}_k) = -[y_1 \log(\hat{y}_1) + y_2 \log(\hat{y}_2)]$$
由于 $y_2 = 1 - y_1$ 且 $\hat{y}_2 = 1 - \hat{y}_1$，带入后就得到了我们熟悉的二元交叉熵损失。

---

## 3. 参数更新（梯度下降）

在 Softmax 交叉熵损失下，通过链式法则推导出的梯度公式依然简洁：

对于第 $k$ 个类别的权重 $w_k$ 的梯度，其形式与二分类逻辑回归惊人地相似：
$$\frac{\partial J}{\partial w_k} = \frac{1}{m} \sum_{i=1}^{m} (\hat{y}_k^{(i)} - y_k^{(i)}) x^{(i)}$$

* 这里的 $(\hat{y}_k^{(i)} - y_k^{(i)})$ 依然是**误差项**。
* $y_k^{(i)}$ 是独热编码的真实标签。

梯度下降依旧遵循 **$w := w - \alpha \cdot \frac{\partial J}{\partial w}$** 的规则，只是现在需要对所有 $K$ 个类别的所有权重矩阵进行更新。

总结来说，**Softmax 交叉熵损失**是逻辑回归解决多分类问题的基石，它通过引入 Softmax 函数实现概率归一化，并通过分类交叉熵来实现对正确类别预测概率最大化的优化目标。

---

