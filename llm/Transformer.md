## 一、Transformer模型
所有Transformer模型（GPT、BERT、T5等）都是作为语言模型进行训练的。这意味着它们是在大量原始文本上以自监督的方式进行训练的。

自监督学习是一种训练方式，其中目标函数可以根据模型的输入自动计算出来。这意味着不需要人工标注数据！

这类模型能够对训练所用的语言形成统计学上的理解，但对于具体的实际任务来说用处不大。因此，通用的预训练模型需要经过一个称为 迁移学习 或微调的过程。在这个过程中，模型会以监督学习的方式进行微调——也就是说，会使用人工标注的标签——以适应特定的任务。

## 二、迁移学习



## Transformer如何解决任务 

### 文本生成




## Transform 架构
### 仅编码器(自编码)
 * Bert 系列
 * 总结：如何通俗地理解BERT的“双向”？
   * 1.不是两个方向分开走，最后再合并 (不像ELMo)，而是从一开始就允许信息在所有单词之间自由流动。 
   * 2.它像是一个解完形填空的高手。在做填空时，你不会只读空格前面的半句话，而是会通读整个句子，利用所有可用信息来做出最佳猜测。BERT就是在做同样的事情。
   * 3.这种深度双向的理解能力，使得BERT在理解一词多义、捕捉长距离依赖关系等方面表现极其出色，从而在众多自然语言处理任务上取得了革命性的进步。

### 仅解码器（自回归）
##### 文本生成任务
##### 
 * Llama 
 * Gemma
 * deepseek-v3
 * 大多数llm
 * 
### 编码器-解码器（序列到序列）

* BART 和 T5 
* mBART、Marian


## LLM推理


## 